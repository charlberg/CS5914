{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31cd2d28-febe-47cb-a9a2-33c50aa0d67d",
   "metadata": {},
   "source": [
    "# Comparative Analysis of REGEX v. Pattern Matching Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456e128-338d-4e18-9461-ddf69414c097",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8f68a58-dbe4-4f73-81ef-57e252af6720",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "#### Regex\n",
    "Regex engines internally use optimized algorithms like Finite State Automata (FSA) or Thompson's NFA to match patterns of varying complexity, from fixed strings to complex rules (e.g., optionality, repetition). \n",
    "Regex processes the entire string using internal algorithms without explicitly managing a \"window.\"  Consequently, time complexity depends on the regex pattern and engine design:\n",
    "For simple patterns: O(n)) with linear scans.\n",
    "For complex patterns: With excessive backtracking, worst-case performance can degenerate to O(2n).\n",
    "\n",
    "#### Algorithms\n",
    "We will review the following algorithms:\n",
    "*  Knuth Morris Pratt\n",
    "*  Rabin Karp\n",
    "*  Boyer Moore\n",
    "*  Wu Manber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe8cf9-3163-43f5-8cba-a51f37adf8dc",
   "metadata": {},
   "source": [
    "Sliding Window Optimization and Heuristics\n",
    "The sliding window optimization involves preprocessing the pattern or string and using a ‘window’ of elements to incrementally move through the string and reusing information to reduce unnecessary comparisons.  \n",
    "The sliding window algorithms achieve efficiency by with Optimization, where hash values are updated as the window slides (e.g., Rabin-Karp), and Heuristics, using precomputed bad character or good suffix tables (e.g., Boyer-Moore).\n",
    "Trie Algorithms\n",
    "A trie approach builds a tree–like data structure to store a set of strings where nodes represent characters and paths represent sets of strings. \n",
    "Tire algorithms are efficient for multi-pattern searching and prefix-based search of large datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fec2effb-8ff4-47ee-a615-ad303b1318d0",
   "metadata": {},
   "source": [
    "## Other String Search & Pattern Matching Algorithms\n",
    "There are other important algorithms which are not dealth with in this workbook\n",
    "#### Aho–Corasick Algorithm\n",
    "Introduced in 1975, the Aho–Corasick algorithm is a multi-pattern string matching algorithm designed to efficiently find instances of multiple patterns in a text simultaneously. It constructs a trie structure of the patterns, augmented with failure links to handle mismatches and allows for seamless transitions between patterns during the search phase. This preprocessing results in a time complexity of O(n+m+k), where n is the text length, m is the total length of all patterns, and k is the number of matches. This algorithm is widely used in applications like text searching, intrusion detection, and bioinformatics.\n",
    "#### Commentz-Walter Algorithm\n",
    "Developed in 1979, the Commentz-Walter algorithm extends the Boyer-Moore algorithm to handle multiple pattern searches efficiently. It combines the concepts of the Boyer-Moore algorithm for individual patterns and the Aho–Corasick algorithm for managing multiple patterns, building a trie structure with precomputed shift tables. This hybrid approach allows for rapid skipping of text sections during mismatches while accommodating multiple patterns. Its time complexity is O(n+m+k), where n is the text length, m is the total length of all patterns, and k is the number of matches, making it suitable for high-performance multi-pattern searching tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5948a-5adf-47b9-8bc5-b788b5d621a6",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00cb18c-22e4-46a6-8b5d-8e45c2b1362c",
   "metadata": {},
   "source": [
    "# String Search with REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ab833b-ebfa-40f7-a680-ff4b917619d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time  # Import the time library\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76a2ee91-ab7d-4d3a-b10f-479051a6b714",
   "metadata": {},
   "source": [
    "Regexes are widely supported in programming languages (Java, Python, Perl) and text processing tools.  Developed in the 1950s by Stephen Cole Kleene for describing regular languages in theoretical computer science, during the 1960's-70's, REGEX was implemented in Unix tools like QED and grep and became integral to text editors and compilers. Regex engines wuse Finite State Automatons (FSA) or Thompson's NFA for matching patterns with performance of O(n) to O(2n).  In contrast to window sliding algorithms, REGEX processes entire the entire string and Features like lookahead/lookbehind assertions, backreferences, and lazy quantifiers allow REGEX to match strings of various complex text processing tasks from exact string search to multi-pattern matching with wildcards and optional characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f68908-3824-43d6-b13c-3a105c467cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 138206 words in the text 'a-tale-of-two-cities.txt'\n",
      "File opened and read in 0.0431 seconds.\n",
      "The 3 search terms appear in this text as follows: \n",
      "\n",
      "| -------- | -------- |\n",
      "|   Word   |  Count   |\n",
      "| -------- | -------- |\n",
      "| birds    |       10 |\n",
      "| women    |       61 |\n",
      "| horses   |       40 |\n",
      "| -------- | -------- |\n",
      "| Total    |      111 |\n",
      "| -------- | -------- |\n",
      "\n",
      "Word count summary completed in 0.0902 seconds.\n",
      "\n",
      "Total execution time: 0.0904 seconds\n"
     ]
    }
   ],
   "source": [
    "# Regex pattern matching solution \n",
    "\n",
    "def file_opener(file_path: str) -> str:\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        word_count = 0\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            match = r\"\\w+\"\n",
    "            for match in re.finditer(match, content):\n",
    "                word_count += 1\n",
    "            print(f\"There are {word_count} words in the text '{file_path}'\")\n",
    "        elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "        print(f\"File opened and read in {elapsed_time:.4f} seconds.\")\n",
    "        return content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"A file error occurred: {e}\")\n",
    "\n",
    "\n",
    "def word_count_summary(file_path: str, search_terms: list[str]) -> None:\n",
    "\n",
    "    start_time = time.time()  \n",
    "    content = file_opener(file_path)\n",
    "\n",
    "    try:\n",
    "        if type(search_terms) == str:  # Input is string, not list\n",
    "            pattern = r'\\b' + search_terms + r'\\b'\n",
    "            frequency = len(re.findall(pattern, content, re.IGNORECASE))\n",
    "            print(f\"The word '{search_terms}' appears {frequency} times.\\n\")\n",
    "        else:\n",
    "            print(f\"The {len(search_terms)} search terms appear in this text as follows: \\n\")  # Input is a list, not string\n",
    "\n",
    "            # Dynamic formatting for table\n",
    "            longest_term = max((term.strip() for term in search_terms), key=len)\n",
    "            column_width = len(longest_term) if len(longest_term) > 8 else 8\n",
    "            line_break = f\"| {'-' * column_width} | {'-' * column_width} |\"\n",
    "            header = \"| {0:^{1}} | {2:^{1}} |\".format('Word', column_width, 'Count')\n",
    "            print(f\"{line_break}\\n{header}\\n{line_break}\")\n",
    "\n",
    "            # Strip terms for multi-term search\n",
    "            word_counter = 0\n",
    "            for word in search_terms:\n",
    "                word = word.strip()\n",
    "                pattern = r'\\b' + word + r'\\b'\n",
    "                frequency = len(re.findall(pattern, content, re.IGNORECASE))\n",
    "                word_counter += frequency\n",
    "                print(f\"| {word:<{column_width}} | {frequency:>{column_width}} |\")  # Output aligned left/right by column\n",
    "\n",
    "            sum_total = f\"| {'Total':<{column_width}} | {word_counter:>{column_width}} |\"\n",
    "            print(f\"{line_break}\\n{sum_total}\\n{line_break}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"A counter error occurred: {e}\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time  # Calculate elapsed time for word count summary\n",
    "    print(f\"Word count summary completed in {elapsed_time:.4f} seconds.\")\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()  # Start the overall timer\n",
    "\n",
    "    file_path = 'a-tale-of-two-cities.txt'\n",
    "    search_terms = ['birds', 'women', 'horses']\n",
    "    word_count_summary(file_path, search_terms)\n",
    "\n",
    "    elapsed_time = time.time() - start_time  # Overall execution time\n",
    "    print(f\"\\nTotal execution time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106582d-05f3-4612-acd0-f6e5efb6dc48",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3905f9-976b-4ed8-80f7-6206e301cd43",
   "metadata": {},
   "source": [
    "# Knuth-Morris-Pratt (KMP) Algorithm\n",
    "## Sliding Window String Search with Optimization  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "521391a8-69ad-486a-9c3b-feb8e6fd2019",
   "metadata": {},
   "source": [
    "Introduced in 1977, the Knuth-Morris-Pratt (KMP) algorithm is a linear-time string matching algorithm designed to efficiently locate substrings within a text. It avoids redundant comparisons by preprocessing the pattern to create a Longest Proper Prefix which is also a Suffix (LPS) table. This table allows the algorithm to skip unnecessary re-evaluations of already matched characters when a mismatch occurs. During the search phase, the algorithm leverages the LPS table to align the pattern dynamically, reducing the overall number of comparisons. With a time complexity of O(n + m), where n is the length of the text and m is the length of the pattern, KMP is particularly effective for scenarios with repetitive patterns or substrings, though it is less suited for handling complex pattern matching tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cba94bd-2cf6-407d-af8d-29ff3c3eaf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in file: a-tale-of-two-cities.txt\n",
      "\n",
      "Results:\n",
      "'birds' appears 10 times. Comparisons: 755864\n",
      "'women' appears 61 times. Comparisons: 755864\n",
      "'horses' appears 40 times. Comparisons: 755864\n",
      "\n",
      "Total comparisons: 2267592\n",
      "Search completed in 0.3477 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Acknowledgement: Code adapted from  \n",
    "# https://gist.github.com/m00nlight/daa6786cc503fde12a77#gistcomment-2750908\n",
    "\n",
    "def compute_lps_array(pattern: str) -> list[int]:\n",
    "    \"\"\"\n",
    "    Computes the Longest Prefix Suffix (LPS) array for the KMP algorithm.\n",
    "    \"\"\"\n",
    "    lps = [0] * len(pattern)\n",
    "    length = 0  # Length of the previous longest prefix suffix\n",
    "    i = 1\n",
    "\n",
    "    while i < len(pattern):\n",
    "        if pattern[i] == pattern[length]:\n",
    "            length += 1\n",
    "            lps[i] = length\n",
    "            i += 1\n",
    "        else:\n",
    "            if length != 0:\n",
    "                length = lps[length - 1]\n",
    "            else:\n",
    "                lps[i] = 0\n",
    "                i += 1\n",
    "\n",
    "    return lps\n",
    "\n",
    "\n",
    "def kmp_search(content: str, pattern: str) -> int:\n",
    "    \"\"\"\n",
    "    Searches for a pattern in the given content using the KMP algorithm.\n",
    "    Returns the frequency of the pattern in the content.\n",
    "    \"\"\"\n",
    "    n = len(content)\n",
    "    m = len(pattern)\n",
    "    lps = compute_lps_array(pattern)\n",
    "    \n",
    "    i = 0  # Index for content\n",
    "    j = 0  # Index for pattern\n",
    "    match_count = 0  # Count of matches\n",
    "    word_comparisons = 0  # Counter for comparisons\n",
    "\n",
    "    while i < n:\n",
    "        word_comparisons += 1  # Increment comparisons counter\n",
    "        if content[i] == pattern[j]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "\n",
    "        if j == m:\n",
    "            match_count += 1\n",
    "            j = lps[j - 1]  # Reset j using the LPS array\n",
    "        elif i < n and content[i] != pattern[j]:\n",
    "            if j != 0:\n",
    "                j = lps[j - 1]\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return match_count, word_comparisons\n",
    "\n",
    "\n",
    "def word_count_summary_kmp(file_path: str, search_terms: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Word frequency analysis with KMP algorithm.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        print(f\"Searching in file: {file_path}\")\n",
    "        start_time = time.time()  # Start timing\n",
    "        total_comparisons = 0  # Counter for total comparisons across all search terms\n",
    "\n",
    "        print(\"\\nResults:\")\n",
    "        for term in search_terms:\n",
    "            frequency, comparisons = kmp_search(content, term)\n",
    "            total_comparisons += comparisons\n",
    "            print(f\"'{term}' appears {frequency} times. Comparisons: {comparisons}\")\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nTotal comparisons: {total_comparisons}\")\n",
    "        print(f\"Search completed in {elapsed_time:.4f} seconds.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please check the file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"a-tale-of-two-cities.txt\"\n",
    "    search_terms = [\"birds\", \"women\", \"horses\"]\n",
    "    word_count_summary_kmp(file_path, search_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46793f8-4c8e-472e-a93c-2ff7e8c209ba",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f80dc0-a75a-412c-b37b-ee13e653a5ed",
   "metadata": {},
   "source": [
    "# Boyer-Moore Algorithm  \n",
    "## Sliding Windome String Search with Heuristics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed5fe169-5caf-4f56-88a1-f26f95006e58",
   "metadata": {},
   "source": [
    "Proposed in 1977, the Boyer-Moore algorithm is a string matching algorithm that achieves high efficiency by skipping sections of the text during the search phase. It leverages two heuristics: the bad character rule and the good suffix rule, which determine how far the pattern can be shifted when a mismatch occurs. These rules significantly reduce the number of comparisons, especially in practical scenarios where the pattern is long or the alphabet is large. The algorithm’s worst-case time complexity is O(n ⋅ m) but it performs close to O(n / m) on average for random text and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2b762f-d855-4bb8-9958-60724436c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the text: 135660\n",
      "Running Boyer-Moore Search...\n",
      "Pattern 'birds' found 10 time(s).\n",
      "Pattern 'women' found 61 time(s).\n",
      "Pattern 'horses' found 40 time(s).\n",
      "\n",
      "Total time taken for Boyer-Moore search: 0.120899 seconds\n",
      "Pattern counts:\n",
      "  'birds': 10 occurrence(s)\n",
      "  'women': 61 occurrence(s)\n",
      "  'horses': 40 occurrence(s)\n"
     ]
    }
   ],
   "source": [
    "# Acknowledgement: Code is adapted from\n",
    "# https://github.com/je-suis-tm/search-and-sort/blob/master/boyer%20moore%20search.py\n",
    "\n",
    "def boyer_moore_search(content, pattern):\n",
    "  \"\"\"\n",
    "  Boyer-Moore algorithm for single pattern matching.\n",
    "  \"\"\"\n",
    "  n = len(content)\n",
    "  m = len(pattern)\n",
    "  if m == 0 or m > n:\n",
    "      return []\n",
    "\n",
    "  # Preprocess the pattern to create the bad character heuristic table\n",
    "  bad_char = dict()  # Create a dictionary for bad character heuristic\n",
    "  for i in range(m):\n",
    "      bad_char[ord(pattern[i])] = i\n",
    "\n",
    "  matches = []\n",
    "  shift = 0\n",
    "\n",
    "  # Slide the pattern over the content\n",
    "  while shift <= n - m:\n",
    "      j = m - 1\n",
    "\n",
    "      # Match pattern from the end\n",
    "      while j >= 0 and content[shift + j] == pattern[j]:\n",
    "          j -= 1\n",
    "\n",
    "      if j < 0:\n",
    "          # Pattern found at this shift\n",
    "          matches.append(shift)\n",
    "          # Move the pattern to the next possible position\n",
    "          shift += (m if (shift + m) < n else 1)  # Move by the full length or 1 if we're at the end of the text\n",
    "      else:\n",
    "          # Use the bad character heuristic to shift\n",
    "          bad_char_shift = bad_char.get(ord(content[shift + j]), -1)  # Handle potential out-of-bounds access using get method of dictionary\n",
    "          shift += max(1, j - bad_char_shift)\n",
    "\n",
    "  return matches\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the text from a file\n",
    "    file_name = \"a-tale-of-two-cities.txt\"\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    patterns = [\"birds\", \"women\", \"horses\"]\n",
    "\n",
    "    total_words = count_words(text)\n",
    "    print(f\"Total words in the text: {total_words}\")\n",
    "\n",
    "    # Run Boyer-Moore search for each pattern\n",
    "    print(\"Running Boyer-Moore Search...\")\n",
    "    total_matches = Counter()\n",
    "    total_time = 0\n",
    "\n",
    "    for pattern in patterns:\n",
    "        start_time = time.time()\n",
    "        matches = boyer_moore_search(text, pattern)\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time += (end_time - start_time)\n",
    "        total_matches[pattern] += len(matches)\n",
    "\n",
    "        print(f\"Pattern '{pattern}' found {len(matches)} time(s).\")\n",
    "\n",
    "    print(f\"\\nTotal time taken for Boyer-Moore search: {total_time:.6f} seconds\")\n",
    "    print(\"Pattern counts:\")\n",
    "    for pattern, count in total_matches.items():\n",
    "        print(f\"  '{pattern}': {count} occurrence(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb1282-a011-4af0-ac49-e320775d4684",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a66be-f614-46e3-9ef4-e94872e93800",
   "metadata": {},
   "source": [
    "# Rabin-Karp Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f19f5ae-8285-4b43-bd7b-d1075193a5f0",
   "metadata": {},
   "source": [
    "Developed in 1987, the Rabin-Karp algorithm is a string matching algorithm that utilizes hashing to efficiently search for patterns within a text. It works by calculating a hash value for the pattern and each substring of the text of the same length, allowing for quick identification of potential matches. In case of a hash match, the algorithm verifies the actual characters to confirm the match, ensuring correctness. While its average-case time complexity is O(n+m), where n is the text length and m is the pattern length, its worst-case complexity is O(n⋅m), particularly if hash collisions occur frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee3ba4d-6451-483d-8a34-ff3157fab791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the text: 135660\n",
      "Searching for pattern: 'birds'\n",
      "Occurrences of 'birds': 10\n",
      "Time taken for Rabin-Karp search: 0.174882 seconds\n",
      "\n",
      "Searching for pattern: 'women'\n",
      "Occurrences of 'women': 61\n",
      "Time taken for Rabin-Karp search: 0.172136 seconds\n",
      "\n",
      "Searching for pattern: 'horses'\n",
      "Occurrences of 'horses': 40\n",
      "Time taken for Rabin-Karp search: 0.173486 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rabin_karp_search(text, pattern, prime=101):\n",
    "    \"\"\"\n",
    "    Rabin-Karp algorithm for pattern matching.\n",
    "    \"\"\"\n",
    "    n = len(text)\n",
    "    m = len(pattern)\n",
    "    d = 256  # Number of characters in the input alphabet\n",
    "    h = 1\n",
    "    p = 0  # Hash value for the pattern\n",
    "    t = 0  # Hash value for the current substring in the text\n",
    "    result = []\n",
    "\n",
    "    # Compute h = d^(m-1) % prime\n",
    "    for _ in range(m - 1):\n",
    "        h = (h * d) % prime\n",
    "\n",
    "    # Compute initial hash values for pattern and first window of text\n",
    "    for i in range(m):\n",
    "        p = (d * p + ord(pattern[i])) % prime\n",
    "        t = (d * t + ord(text[i])) % prime\n",
    "\n",
    "    # Slide the pattern over the text one character at a time\n",
    "    for i in range(n - m + 1):\n",
    "        # Check if hash values match\n",
    "        if p == t:\n",
    "            # Verify characters one by one\n",
    "            if text[i:i + m] == pattern:\n",
    "                result.append(i)\n",
    "\n",
    "        # Compute hash for the next substring\n",
    "        if i < n - m:\n",
    "            t = (d * (t - ord(text[i]) * h) + ord(text[i + m])) % prime\n",
    "\n",
    "            # Handle negative hash value\n",
    "            if t < 0:\n",
    "                t += prime\n",
    "\n",
    "    return result\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_name = \"a-tale-of-two-cities.txt\"\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    patterns = ['birds', 'women', 'horses']\n",
    "\n",
    "    word_count = count_words(text)\n",
    "    print(f\"Total words in the text: {word_count}\")\n",
    "\n",
    "    for pattern in patterns:\n",
    "        print(f\"Searching for pattern: '{pattern}'\")\n",
    "        start_time = time.time()\n",
    "        matches = rabin_karp_search(text, pattern)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"Occurrences of '{pattern}': {len(matches)}\")\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Time taken for Rabin-Karp search: {elapsed_time:.6f} seconds\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c5d00-7653-415c-929f-0a44700eb878",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8cc71b-4b75-440d-8ff0-d132b8cb6997",
   "metadata": {},
   "source": [
    "# Wu-Manber Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b43f2d-efb8-409c-816f-bc5cf7cda793",
   "metadata": {},
   "source": [
    "Proposted in 1994, Wu-Manber algorithm is highly useful for finding words or patterns in texts, especially when searching for multiple patterns simultaneously. It is a multi-pattern string matching algorithm designed for high-speed searching in large-scale texts where efficiency is critical. it is optimized for exact string matching but can be adapted for approximate matching with small modifications. Similar to the Boyer-Moore algorithm, it leverages hash tables and shift tables to handle multiple patterns simultaneously. The algorithm preprocesses the patterns to compute a shift table and uses a hashing mechanism to identify candidate matches quickly. Importantly, it minimizes the amount of text scanned during mismatches, making it particularly well-suited for applications like text indexing, network intrusion detection, and digital forensics. The Wu-Manber algorithm achieves a time complexity of 𝑂(𝑛) on average, where 𝑛 is the text length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed586ba6-9012-4cfc-b8c4-4a2803ee243e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the text: 135660\n",
      "Pattern 'birds' found 10 time(s).\n",
      "Pattern 'women' found 61 time(s).\n",
      "Pattern 'horses' found 40 time(s).\n",
      "Time taken for Wu-Manber search: 0.157471 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "def wu_manber_search(content, patterns):\n",
    "    \"\"\"\n",
    "    Wu-Manber search algorithm for multiple pattern matching\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocessing - Build the hash table\n",
    "    shift_table = {}\n",
    "    min_pattern_length = min(len(pattern) for pattern in patterns)\n",
    "\n",
    "    # Populate shift table for all patterns\n",
    "    for pattern in patterns:\n",
    "        for i in range(len(pattern) - min_pattern_length + 1):\n",
    "            substring = pattern[i:i + min_pattern_length]\n",
    "            if substring not in shift_table:\n",
    "                shift_table[substring] = []\n",
    "            shift_table[substring].append(pattern)\n",
    "\n",
    "    # Step 2: Search in text\n",
    "    matches = []\n",
    "    i = 0\n",
    "    while i <= len(content) - min_pattern_length:\n",
    "        substring = content[i:i + min_pattern_length]\n",
    "        if substring in shift_table:\n",
    "            # Verify each candidate pattern\n",
    "            for candidate in shift_table[substring]:\n",
    "                if content[i:i + len(candidate)] == candidate:\n",
    "                    matches.append((i, candidate))\n",
    "        i += 1\n",
    "\n",
    "    return matches\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Example Usage with Timer and Pattern Count\n",
    "if __name__ == \"__main__\":\n",
    "    file_name = \"a-tale-of-two-cities.txt\"\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    patterns = [\"birds\", \"women\", \"horses\"]\n",
    "\n",
    "    total_words = count_words(text)\n",
    "    print(f\"Total words in the text: {total_words}\")\n",
    "\n",
    "    # Run Wu-Manber search\n",
    "    start_time = time.time()  # Start timer\n",
    "    results = wu_manber_search(text, patterns)\n",
    "    end_time = time.time()  # End timer\n",
    "\n",
    "    # Count occurrences of each pattern\n",
    "    pattern_counts = Counter([match[1] for match in results])\n",
    "\n",
    "    # Print results\n",
    "    if results:\n",
    "        for pattern in patterns:\n",
    "            count = pattern_counts.get(pattern, 0)\n",
    "            print(f\"Pattern '{pattern}' found {count} time(s).\")\n",
    "    else:\n",
    "        print(\"No patterns found.\")\n",
    "\n",
    "    # Print elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time taken for Wu-Manber search: {elapsed_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4740d7-167a-4eb2-8fc6-bf59987b32d1",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bcf935-971a-4c09-a70a-b17c08114cc5",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce613f9-8a81-498d-9643-f558fab9a91b",
   "metadata": {},
   "source": [
    "For the literary texts used in this exercies, the window shifting KMP and Boyer Moore algorithms outperformed the regex pattern search, while the exact search and trie algorithms were slower.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6681c-0c05-4f86-901d-37ebb364e947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
